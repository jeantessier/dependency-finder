I've been thinking about how to deal with very large projects that Dependency
Finder cannot fit into memory.  I could partition the dependency graph between
multiple servers, query them in parallel, and merge the results together.  I
already have the merge logic.  Next, I need a way to partition the graph
efficiently.

I could do preprocessing of the codebase to determine the partitions
automatically.  I could use a variation on LCOM4 for this.  Well, not really
LCOM4, but more of a general graph partitioning algorithm.  I could generate a
directed package-to-package graph with a weight on each edge that tells how many
classes are involved in that edge.  Then, I do a normal graph partitioning but I
only consider edges with weight greater than _N_.  By varying _N_, I can find
the optimal partitions, including which packages are in each one.

To do the preprocessing, I could simply have a =DependencyListener= listen in
and tally the dependencies.  But =CodeDependencyCollector=, which triggers the
listeners, is way too tightly coupled to =NodeFactory=.  I would like to run
=CodeDependencyCollector= without a =NodeFactory= so it would only generate
=DependencyEvent= messages, and those would have enough information to
distinguish between different kinds of dependencies.  I could use some kind of
non-caching =NodeFactory=, or rather have some other class create the
dependencies so I can have a very lightweight graph in the factory while I just
tally information about the dependencies but not the dependencies themselves for
this specific analysis.

See also
[[http://www.cs.sunysb.edu/~algorith/files/graph-partition.shtml][graph partition algorithms]]
at [[http://www.cs.sunysb.edu/~algorith/][The Stony Brook Algoritm Repository]].
